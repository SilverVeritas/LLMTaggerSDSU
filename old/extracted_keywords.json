{
  "Counterfactual Explanations for k-means and Gaussian Clustering.pdf": {
    "response": "Counterfactual explanations  \nk-means clustering  \nGaussian clustering  \nExplainable AI  \nLocal explanation methods  \nGlobal explanation methods  \nPlausibility constraints  \nFeasibility constraints  \nEuclidean distance  \nActionable features  \nImmutable features  \nCluster boundary  \nNumerical solution  \nNonlinear equation  \nSynthetic datasets  \nReal datasets  \nExperimental comparisons  \nMachine learning models  \nTrustworthy and transparent  \nDecision trees  \nSaliency maps  \nClassification decisions  \nCluster assignment  \nProbability density  \nCluster centers  \nCovariance matrices  \nCluster density  \nCluster model  \nFeature actionability  \nPlausibility factor  \nOptimization problem  \nQuadratic minimization  \nLinear constraint  \nCluster boundary  \nGaussian distribution  \nMahalanobis distance  \nCluster separability  \nCluster fairness"
  },
  "Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based.pdf": {
    "response": "Preprint Version  \nICAACE 2025  \nDetection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models  \nZizhou Zhang  \nXinshi Li  \nYu Cheng  \nZhenrui Chen  \nQianying Liu  \nCredit Risk Identification in Supply Chains Using Generative Adversarial Networks  \nGenerative Adversarial Networks (GANs)  \nSupply Chain Risk  \nCredit Risk Identification  \nMachine Learning  \nData Augmentation  \nSupply chain risk management  \nCredit risk contagion  \nManufacturing (steel)  \nDistribution (pharmaceuticals)  \nServices (e-commerce)  \nLogistic regression  \nDecision trees  \nNeural networks  \nRecurrent Neural Networks (RNNs)  \nLong Short-Term Memory (LSTM) networks  \nWasserstein GAN (WGAN)  \nBatch normalization  \nLabel smoothing  \nAdam optimizer  \nAccuracy  \nPrecision  \nRecall  \nF1-score  \nReceiver Operating Characteristic (ROC) curve  \nArea under the curve (AUC)  \nSynthetic credit risk data  \nProactive risk management  \nMacroeconomic indicators  \nMarket trends  \nSupplier relationship dynamics"
  },
  "Grey-Box Fuzzing in Constrained Ultra-Large Systems Lessons.pdf": {
    "response": "Grey-Box Fuzzing\nConstrained Ultra-Large Systems\nSandBoxFuzz\nAspect-Oriented Programming\nRuntime Reflection\nDynamic Specification Mining\nLog-Based Coverage Mechanism\nMicroservices-Based FinTech Systems\nAnt Group\nBranch Coverage\nExceptions\nSetup Time Reduction\nOpen-Sourcing\nTesting Ultra-Large Systems\nSoftware Vulnerabilities\nFuzz Testing\nIndustrial Settings\nLimited Permissions\nRestricted Test Environments\nInput Constructions\nCoverage-Guided Fuzzing\nBytecode Instrumentation\nSpecification Mining\nSandbox Interception\nMutation Space\nManual Effort Reduction\nScalable Deployment\nReal-World Industrial Environment"
  },
  "Hybrid Deep Learning Model for epileptic seizure.pdf": {
    "response": "Hybrid Deep Learning Model  \nEpileptic Seizure Classification  \n1D-CNN  \nMulti-Head Attention Mechanism  \nElectroencephalography (EEG) Signals  \nWavelet Transform  \nConvolutional Neural Networks (CNNs)  \nSpatial Patterns  \nTemporal Aspects  \nRegularization Techniques  \nDropout Layers  \nL2 Regularization  \nEarly Stopping  \nClassification Accuracy  \nAutomated Tools  \nEpileptic Seizure Detection  \nDeep Learning Techniques  \nTime-Series Data  \nFeature Extraction  \nPrincipal Component Analysis (PCA)  \nChi-square  \nFeature Scaling  \nDimensionality Reduction  \nStandard Scaling  \nMinMax Scaling  \nUCI Epileptic Seizure Dataset  \nBinary Classification  \nFeature Scaling  \nStandardScaler  \nPerformance Evaluation Metrics  \nAccuracy  \nPrecision  \nRecall  \nF1 Score  \nCritical Success Index (CSI)  \nMatthews Correlation Coefficient (MCC)  \nWavelet Transform  \n1D Convolutional Layers  \nBatch Normalization  \nMaxPooling  \nSkip Connection  \nGlobal Average Pooling  \nFully Connected Layers  \nAdam Optimizer  \nBinary Cross-Entropy  \nL2 Regularization  \nDaubechies 1 (db1) Wavelet  \nMulti-Head Attention Layer  \nAttention Weights  \nResidual Connection  \nSeizure Detection  \nBiomedical Signal Processing  \nElectroencephalogram (EEG)  \nElectrocardiogram (ECG)  \nElectromyography (EMG)  \nAdaptive Wavelet Analysis  \nHarmonic Component Modeling  \nDenoising  \nDecomposition  \nAdaptive Segmentation  \nScalogram Images  \nLeast Square Support Vector Machine (LSSVM)  \nNoise Removal  \nTime-Frequency Analysis  \nDeep Canonical Sparse Autoencoder  \nCoyote Optimization Algorithm (COA)  \nKrill Herd Algorithm (KHA)  \nAttention Mechanisms  \nPre-Trained Convolution Capsule Networks  \nFeature Selection  \nMachine Learning Algorithms  \nNeural Network Systems  \nCoefficient of Variation  \nStock Price Prediction  \nFeature Scaling Techniques  \nReal-Time EEG-Based Emotion Recognition  \nTree-Based Algorithms  \nAdaptive Filters  \nTime-Varying Wave-Shape Model  \nBiomedical Signal Processing  \nDeep Learning Architectures  \nSignal Processing Techniques  \nImage Classification  \nNatural Language Processing  \nMotor Imagery EEG Classification  \nSupport Vector Machines (SVMs)  \nK-Nearest Neighbors (KNN)  \nRandom Forests (RF)  \nDecision Trees (DT)  \nEpileptic Seizure Detection Systems  \nAutomated Diagnostic System  \nEpileptic Seizure Prediction  \nMachine Learning Approaches  \nDeep Learning Approaches  \nEpileptic Seizure Dataset  \nEpileptic Seizure Recognition  \nFeature Scaling  \nDropout Layers  \nHybrid Convolutional Neural Network  \nGated Recurrent Unit Model  \nLightGBM Classifier  \nFunctional Iterative Approach  \nUniversum-Based Primal Twin Bounded Support Vector Machine"
  },
  "Mapping the Increasing Use of LLMs in Scientific Papers.pdf": {
    "response": "Preprint\nMapping the Increasing Use of LLMs in Scientific Papers\nStanford University\nUC Santa Barbara\nScientific publishing\nLarge language models (LLMs)\nChatGPT\nAcademic writing\nGlobal scientific practices\nSystematic large-scale analysis\nPopulation-level statistical framework\nLLM-modified content\nComputer Science papers\nMathematics papers\nNature portfolio\nPreprints\nCrowded research areas\nShorter lengths\nAnecdotal examples\nPeer reviews\nChatGPT-generated\nDetection method\nCorpus\nEpistemic shifts\nLinguistic shifts\nAccuracy\nPlagiarism\nAnonymity\nOwnership\nInternational Conference on Machine Learning (ICML)\nEditorial policies\nScientific publishing ecosystem\nAuthor behavior\nStructural challenges\nPublish or perish\nLinguistic discrimination\nProse editors\nDistributional GPT quantification framework\nAI-modified content\nAbstractive summarization\nFull vocabulary\nWord frequency shift\nTemporal distribution shift\nModel accuracy\nCalibration\nTemporal trends\nAI-modified academic writing\nPreprint posting frequency\nPaper similarity\nEmbedding distance\nPaper length\nCompetitive nature\nResearch areas\nTime constraints\nSecurity\nIndependence\nScientific practice\nTransparent publishing\nEpistemically diverse\nAccurate\nIndependent\nScientific publishing\nChatGPT\nGenerative AI\nTraining data\nLLM pretraining\nData quality\nStereotypes\nBiases\nModel collapse\nData curation\nFiltering strategies"
  },
  "MechIR A Mechanistic Interpretability Framework.pdf": {
    "response": "MechIR  \nMechanistic Interpretability  \nInformation Retrieval  \nExplainability  \nMachine Learning  \nneural models  \ndiagnostic analysis  \nintervention  \nhighly parametric neural systems  \nIR tasks  \naxiomatic lens  \nactivation patching  \nTransformerLens  \ncausal interventions  \nneural networks  \nreverse-engineer IR models  \ninternal model components  \nbi-encoders  \ncross-encoders  \nrelevance score  \nquery and document representations  \nTransformer-based language models  \nIR datasets  \nPyTerrier  \nperturbation  \nopen-source Python package  \nexplainable information retrieval (XIR)  \nneural search  \nrecommender systems  \ndiagnostic tools  \nmodel performance  \nmitigate bias  \nadversarial attacks  \nsteerable and personalized systems"
  },
  "Modelling Activity Scheduling Behaviour with Deep.pdf": {
    "response": "Modelling Activity Scheduling Behaviour  \nDeep Generative Machine Learning  \nVariational Auto-Encoders  \nActivity schedule encoding  \nContinuous encoding of activity durations  \nComprehensive evaluation framework  \nHuman Behavioural Modelling  \nChoice Modelling  \nActivity-Based Models  \nMATSim  \nTransport, energy and epidemiology domains  \nParticipation and timing  \nHigh-dimensional object  \nComplex joint distributions  \nPhysical and temporal constraints  \nPersonal and social constraints  \nIndividual preferences  \nInteractions with others  \nSequential choices  \nRule based scheduling algorithms  \nDeep generative learning  \nSynthesis  \nData anonymisation  \nUp-sampling  \nObject detection  \nTranslation  \nText generation  \nRegular-human performance  \nDecision support tools  \nTransport, energy and epidemiology  \nBespoke-rules  \nStatistical choice models  \nNested-logit models  \nHazard models  \nMultinomial-logit models  \nMarkov Chain Monte Carlo  \nSimultaneous model  \nSimulated and sampled choice set  \nComputationally expensive  \nScalability  \nTour-based approaches  \nDiscriminative machine learning  \nProfile Hidden Markov Models  \nAggregate distributions  \nUK National Travel Survey  \nDeep Generative Models (DGMs)  \nAuto-regressive models  \nRecurrent Neural Networks (RNNs)  \nVariational Auto Encoders (VAEs)  \nLatent variable model  \nGenerative Adversarial Networks (GANs)  \nNormalising flows  \nDensity estimation  \nProbability density estimation  \nLikelihood  \nDownstream classification models  \nHuman feedback  \nPopulation synthesis  \nJoint attributes  \nFeasibility-diversity trade-off  \nDiscriminative model  \nRecurrent neural networks (RNNs)  \nAttention based architecture  \nDifferentiable privacy  \nSmart card data  \nVariance in model predictions  \nTransformer architectures  \nTemporal and spatial encodings  \nLanguage models  \nSequences of tokens  \nLong-range dependencies  \nKL divergence  \nMixed data type  \nCategorical information  \nContinuous information  \nNovel approach  \nActivity schedule encoding  \nContinuous encoding of activity durations  \nComprehensive evaluation framework  \nRapid synthesis  \nBespoke Variational Auto-Encoders architecture  \nProbability density  \nReal sample  \nSynthetic samples  \nDensity estimation  \nSample quality  \nCreativity  \nEarth Movers Distance (EMD)  \nL1 distance  \nAggregate  \nParticipation  \nTransitions  \nTiming  \nHome based  \nSequence duration  \nDiversity  \nHomogeneity  \nNovelty  \nConservatism  \nDiscrete Schedule Encoding  \nContinuous Schedule Encoding  \nDiscrete CNN  \nContinuous RNN  \nConvolutional network  \nRecurrent network  \nAuto-regressive predictions  \nImage-based model  \nTop-down prediction  \nReconstruction loss  \nKullback-Leibler Divergence (KLD)  \nCross-entropy  \nMean squared error  \nTeacher forcing  \nAggregate Frequency  \nActivity Participation  \nActivity Transition  \nActivity Timing  \nActivity Durations  \nSample Quality  \nCreativity  \nDiversity  \nNovelty  \nPracticalities  \nStochasticity  \nCaveat  \nDeep generative approach  \nHuman behavioural system  \nHuman preferences  \nStructural restrictions  \nTemporal consistency  \nAggregate realism  \nDisaggregate realism  \nGenerative approach  \nDiscriminative models  \nBespoke logic  \nNon-conditioned generation  \nConditionality  \nGenerative process  \nMultiple days  \nAdditional activity types  \nTrips  \nLocation  \nTravel mode choice"
  },
  "SEANN A Domain-Informed Neural Network for.pdf": {
    "response": "arXiv:2501.10273v1  \nSEANN  \nDomain-Informed Neural Network  \nEpidemiological Insights  \nJean-Baptiste Guimbaud  \nMarc Plantevit  \nLIRIS  \nISGlobal  \nMeersens  \nEPITA Research Laboratory  \nlogistic regression  \nlinear regression  \nparametric models  \nnon-parametric machine learning  \ndeep neural networks (DNNs)  \nexplainable AI (XAI)  \nhigh-quality data  \nhigh-quantity data  \nPooled Effect Sizes (PES)  \nMeta-Analysis  \nscientific consensus  \ncustom loss  \ngeneralizability  \npredictive performances  \nscientific plausibility  \nexposome paradigm  \nenvironmental exposures  \nurban, chemical, lifestyle, social hazards  \nhigh-dimensional data  \nobservational studies  \ninterpretability  \ndata-driven approaches  \nbiological pathways  \nInformed Machine Learning (IML)  \ndomain-specific knowledge  \nPhysics-Guided Neural Network (PGNN)  \nDomain-Adapted Neural Network (DANN)  \nCArdiovascular LIterature-Based Risk Algorithm (CALIBRA)  \nSummary Effects Adapted Neural Network  \nOdd Ratios (ORs)  \nRelative Risks (RRs)  \nStandard Regression Coefficients (SRCs)  \ncustom loss function  \nperturbation  \ngeneralizability  \ntrustworthiness  \nShapley values  \ncoefficient of determination (R2)  \nreceiver operating characteristic curve (ROC)  \narea under the curve (AUC)  \nGaussian noise  \nmean imputation  \nearly stopping  \nPyTorch  \nAdam optimization  \nSHAP library  \nHELIX project  \nHuman Early Life Exposome (HELIX) study"
  },
  "The Distributed Multi-User Point Function.pdf": {
    "response": "Distributed Multi-User Point Function  \nInformation-theoretic distributed multi-user point function  \nTrusted master node  \nServer nodes  \nUsers  \nPoint function  \nFunctional shares  \nCorrectness and information-theoretic privacy constraints  \nCapacity  \nMaximum achievable rate  \nStorage size  \nNovel converse argument  \nDistributed Systems  \nMulti-User Secrecy  \nMulti-User Secret Sharing  \nCloud computing  \nBlockchain technology  \nDistributed learning  \nStorage services  \nSecurity and privacy concerns  \nCryptographic primitives  \nSecret-sharing schemes  \nInformation-theoretic security  \nFunction secret sharing schemes  \nComputational privacy  \nInformation-theoretic privacy  \nPrivate information retrieval (PIR)  \nDistributed key generation (DKG)  \nNon-linear and linear reconstruction methods  \nCommunication and storage complexity  \nQuery conversion  \nVerifiable information-theoretic secure scheme  \nMulti-user or multi-secret sharing  \nDistributed multi-secret or multi-user secret sharing (DMUSS)  \nWeak secrecy conditions  \nPerfect secrecy  \nCapacity region  \nGraph-based secret sharing  \nCorrelated random variables  \nSymmetric and multilevel secret sharing schemes  \nDistributed Multi-User Point Function (DMUPF)  \nPlacement phase  \nDemand phase  \nEvaluation phase  \nRetrieval phase  \nCorrectness condition  \nPrivacy condition  \nCapacity region of DMUPF  \nAchievable scheme  \nInner bound  \nOuter bound  \nVerifiable information-theoretic point function"
  },
  "The_Evolution_of_Large_Language_Model_Models_Applications_and_Challenges.pdf": {
    "response": "Large Language Models (LLMs)  \nNatural Language Processing (NLP)  \nTransformer architecture  \nBERT  \nGPT  \nT5  \nBART  \nBioGPT  \nAccuracy  \nPerplexity  \nBLEU score  \nROUGE score  \nBias  \nOverfitting  \nFederated learning  \nContinuous learning  \nMultimodal LLMs  \nInterpretive AI  \nText synthesis  \nTranslation  \nSummarization  \nQuestion-answering  \nSentiment analysis  \nRecurrent Neural Networks (RNNs)  \nSelf-attention mechanism  \nPre-trained models  \nFine-tuning  \nContext length improvements  \nEthics  \nPrivacy  \nEnvironmental concerns  \nBiomedical  \nHealthcare  \nEducation  \nSocial media  \nBusiness  \nAgriculture  \nTransformer-based models  \nModel scaling  \nPre-trained language models  \nText generation  \nChatbots  \nContent generation  \nRobo-advising  \nAlgorithmic trading  \nHuman-robot interaction  \nReal-time processing  \nLong-term dependencies  \nPrompt engineering  \nRetrieval-augmented generation  \nDynamic evaluation metrics  \nPersonalization  \nEthical frameworks"
  }
}