{
  "Counterfactual Explanations for k-means and Gaussian Clustering.pdf": {
    "response": "Counterfactual explanations  \nk-means clustering  \nGaussian clustering  \nExplainable AI  \nPlausibility constraints  \nFeasibility constraints  \nEuclidean distance  \nActionable features  \nImmutable features  \nPlausibility factor  \nAnalytical mathematical formulas  \nNonlinear equation  \nSynthetic datasets  \nReal datasets  \nGlobal explanation methods  \nLocal explanation methods  \nSaliency maps  \nMachine learning models  \nCluster boundary  \nCluster density  \nCluster assignment  \nProbability density  \nCluster centers  \nCovariance matrices  \nCluster model  \nFeature actionability  \nQuadratic optimization problem  \nLagrange multipliers  \nProjection  \nCluster separability  \nCluster fairness"
  },
  "Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based.pdf": {
    "response": "Preprint Version  \nICAACE 2025  \nDetection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models  \nZizhou Zhang  \nXinshi Li  \nYu Cheng  \nZhenrui Chen  \nQianying Liu  \nCredit Risk Identification in Supply Chains Using Generative Adversarial Networks  \nGenerative Adversarial Networks (GANs)  \nSupply Chain Risk  \nCredit Risk Identification  \nMachine Learning  \nData Augmentation  \nSupply chain risk management  \nCredit risk contagion  \nLogistic regression  \nDecision trees  \nNeural networks  \nRecurrent Neural Networks (RNNs)  \nLong Short-Term Memory (LSTM) networks  \nWasserstein GAN (WGAN)  \nBatch normalization  \nLabel smoothing  \nAdam optimizer  \nReceiver Operating Characteristic (ROC) curve  \nSynthetic credit risk data  \nProactive risk management  \nMacroeconomic indicators  \nMarket trends  \nSupplier relationship dynamics  \nSupply chain finance  \nIEEE  \n8th International Conference on Advanced Algorithms and Control Engineering (ICAACE 2025)"
  },
  "Grey-Box Fuzzing in Constrained Ultra-Large Systems Lessons.pdf": {
    "response": "Grey-Box Fuzzing\nConstrained Ultra-Large Systems\nSandBoxFuzz\nAspect-Oriented Programming\nRuntime Reflection\nDynamic Specification Mining\nLog-Based Coverage Mechanism\nMicroservices-Based FinTech Systems\nAnt Group\nBranch Coverage\nExceptions\nSetup Time Reduction\nOpen-Sourcing\nIndustrial Environment\nTesting Ultra-Large Systems\nSoftware Vulnerabilities\nFuzz Testing\nAutomated Technique\nSecurity Vulnerabilities\nIndustrial Settings\nLimited Permissions\nRestricted Test Environments\nInput Constructions\nCoverage-Guided Fuzzer\nJava\nBytecode Augmentation\nControl-Flow Graph\nSpecification Mining\nSandbox Interception\nJVM-Sandbox\nMutation Space\nManual Effort\nScalable Deployment\nReal-World Microservices\nContinuous Integration and Deployment (CI/CD)\nTest Frameworks\niTest\nTest Lifecycle\nFile-Based Formats\nTest Inputs\nTest Environment\nCold Start Overhead\nDynamic Mutation\nCoverage Feedback\nSeed Pool\nFuzzing Iterations\nExecution Logs\nTarget Exceptions\nManual Test Suite\nFuzzing Effectiveness\nPerformance Evaluation\nOverhead\nScalability\nTemporal Variability\nInstrumentation Failures\nExternal Validity\nMicroservices Architectures\nLarge-Scale Distributed Systems"
  },
  "Hybrid Deep Learning Model for epileptic seizure.pdf": {
    "response": "Hybrid Deep Learning Model  \nEpileptic Seizure Classification  \n1D-CNN  \nMulti-Head Attention Mechanism  \nElectroencephalography (EEG) Signals  \nWavelet Transform  \nConvolutional Neural Networks (CNNs)  \nSpatial Patterns  \nTemporal Aspects  \nRegularization Techniques  \nDropout Layers  \nL2 Regularization  \nEarly Stopping  \nClassification Accuracy  \nBenchmark Datasets  \nAutomated Tools  \nSeizure Detection  \nDeep Learning Techniques  \nTime-Series Data  \nFeature Extraction  \nPrincipal Component Analysis (PCA)  \nChi-square  \nFeature Scaling  \nDimensionality Reduction  \nStandardScaler  \nStandard Scaling  \nMinMax Scaling  \nUCI Epileptic Seizure Dataset  \nBinary Classification  \nFeature Scaling  \nStandardScaler  \nPerformance Evaluation Metrics  \nAccuracy  \nPrecision  \nRecall  \nF1 Score  \nCritical Success Index (CSI)  \nMatthews Correlation Coefficient (MCC)  \nSingle-Level Wavelet  \nDaubechies 1 (db1) Wavelet  \n1D Convolutional Neural Network (CNN)  \nBatch Normalization  \nMaxPooling  \nSkip Connection  \nGlobal Average Pooling  \nFully Connected Layers  \nDropout  \nAdam Optimizer  \nBinary Cross-Entropy  \nAttention Layer  \nResidual Connection  \nEEG Signal Analysis  \nSeizure Detection Systems  \nClinical Decision-Making  \nBiomedical Signal Processing  \nElectrocardiogram (ECG)  \nElectromyography (EMG)  \nAdaptive Wavelet Analysis  \nHarmonic Component Modeling  \nDenoising  \nDecomposition  \nAdaptive Segmentation  \nScalogram Images  \nLeast Square Support Vector Machine (LSSVM)  \nNoise Removal  \nTime-Frequency Analysis  \nDeep Canonical Sparse Autoencoder (DCSAE)  \nCoyote Optimization Algorithm (COA)  \nKrill Herd Algorithm (KHA)  \nAttention Mechanisms  \nEmotion Recognition  \nPre-Trained Convolution Capsule Networks  \nFeature Selection  \nMachine Learning Algorithms  \nNeural Network Performance  \nFeature Scaling Techniques  \nDeep Learning Architecture  \nEpileptic Seizure Detection  \nClinical Application  \nBiomedical Signals  \nSignal Processing Techniques  \nDeep Learning Models  \nNeural Networks  \nSignal Classification  \nEEG Data  \nSeizure Recognition  \nTemporal Dependencies  \nSpatial Features  \nTemporal Features  \nModel Generalization  \nOverfitting Prevention  \nModel Robustness  \nModel Performance  \nModel Evaluation  \nModel Implementation  \nModel Training  \nModel Testing  \nModel Accuracy  \nModel Comparison  \nState-of-the-Art Methods  \nProposed Architecture  \nSeizure Detection Methodology  \nData Preprocessing  \nData Cleaning  \nData Normalization  \nData Points  \nSignal Patterns  \nSignal Segments  \nSignal Characteristics  \nSignal Features  \nSignal Analysis  \nSignal Interpretation  \nSignal Variations  \nSignal Complexity  \nSignal Noise  \nSignal Dependencies  \nSignal Recognition  \nSignal Classification  \nSignal Detection  \nSignal Processing  \nSignal Extraction  \nSignal Transformation  \nSignal Reduction  \nSignal Scaling  \nSignal Evaluation  \nSignal Metrics  \nSignal Performance  \nSignal Techniques  \nSignal Models  \nSignal Systems  \nSignal Applications  \nSignal Approaches  \nSignal Methods  \nSignal Architectures  \nSignal Components  \nSignal Mechanisms  \nSignal Connections  \nSignal Layers  \nSignal Networks  \nSignal Data  \nSignal Tasks  \nSignal Problems  \nSignal Solutions  \nSignal Challenges  \nSignal Opportunities  \nSignal Trends  \nSignal Advances  \nSignal Developments  \nSignal Innovations  \nSignal Improvements  \nSignal Enhancements  \nSignal Contributions  \nSignal Findings  \nSignal Insights  \nSignal Impacts  \nSignal Implications  \nSignal Benefits  \nSignal Limitations  \nSignal Considerations  \nSignal Evaluations  \nSignal Comparisons  \nSignal Analyses  \nSignal Discussions  \nSignal Conclusions  \nSignal References"
  },
  "Mapping the Increasing Use of LLMs in Scientific Papers.pdf": {
    "response": "Preprint\nMapping the Increasing Use of LLMs in Scientific Papers\nStanford University\nUC Santa Barbara\nScientific publishing\nLarge language models (LLMs)\nChatGPT\nAcademic writing\nGlobal scientific practices\nSystematic large-scale analysis\narXiv\nbioRxiv\nNature portfolio journals\nPopulation-level statistical framework\nLLM-modified content\nComputer Science papers\nMathematics papers\nNature portfolio\nPreprints\nCrowded research areas\nShorter lengths\nAnecdotal examples\nPeer reviews\nChatGPT-generated\nDetection method\nLLM-modified text\nCorpus\nEpistemic shifts\nLinguistic shifts\nAccuracy\nPlagiarism\nAnonymity\nOwnership\nInternational Conference on Machine Learning (ICML)\nJournal Science\nEditorial policies\nScientific publishing ecosystem\nAuthor behavior\nStructural challenges\nPublish or perish\nLinguistic discrimination\nProse editors\nDistributional GPT quantification framework\nAI-modified content\nAbstractive summarization\nFull vocabulary\nWord frequency shift\nLog odds ratio\nPopulation level\nTemporal distribution shifts\nRobustness\nComputational efficiency\nAcademic disciplines\nEmbedding space\nWriting diversity\nGPT Detectors\nZero-shot approaches\nTraining-based methods\nBinary classification\nHuman vs. LLM-modified text\nAdversarial attacks\nNon-dominant language varieties\nInstance-level detection\nDistributional LLM quantification framework\nProbability distributions\nMixture distribution\nToken occurrences\nParameterization\nEstimation\nInference\nMLE\nRealistic LLM-produced training data\nTwo-stage approach\nCounterfactual framework\nFull vocabulary\nSample-efficient\nData collection\nSampling\nModel fitting\nEvaluation\nTemporal distribution shift\nValidation data\nPrediction error\nTemporal trends\nAI-modified academic writing\nPre-ChatGPT reference point\nFirst-author preprint posting frequency\nPaper similarity\nEmbedding distance\nResearch fields\nPaper length\nWord count\nAppendices\nCompetitive nature\nResearch areas\nTime constraints\nAI writing assistance\nSecurity\nIndependence\nScientific practice\nTransparent\nEpistemically diverse\nAccurate\nIndependent scientific publishing\nLimitations\nOther large language models\nFalse positives\nMultilingual scholars\nLanguage-use shifts\nCausal studies\nAcknowledgments\nReferences"
  },
  "MechIR A Mechanistic Interpretability Framework.pdf": {
    "response": "MechIR  \nMechanistic Interpretability  \nInformation Retrieval  \nExplainability  \nMachine Learning  \nneural models  \ndiagnostic analysis  \nintervention  \nhighly parametric neural systems  \nIR tasks  \naxiomatic lens  \nactivation patching  \nTransformerLens  \ncausal interventions  \nneural networks  \nopen-source Python package  \nbi-encoders  \ncross-encoders  \nrelevance score  \nattention heads  \nMLP layer  \nIR datasets  \nPyTerrier  \nperturbation  \nTREC DL19  \nDL20 test collection  \nranking axioms  \nexplainable information retrieval (XIR)  \nneural search  \nrecommender systems  \ndiagnostic tools  \nmodel performance  \nbias mitigation  \nadversarial attacks  \nsteerable systems  \npersonalized systems"
  },
  "Modelling Activity Scheduling Behaviour with Deep.pdf": {
    "response": "Modelling Activity Scheduling Behaviour  \nDeep Generative Machine Learning  \nVariational Auto-Encoders  \nActivity schedule encoding  \nContinuous encoding of activity durations  \nComprehensive evaluation framework  \nHuman Behavioural Modelling  \nChoice Modelling  \nActivity-Based Models  \nMATSim  \nParticipation and timing  \nHigh-dimensional object  \nComplex joint distributions  \nPhysical and temporal constraints  \nPersonal and social constraints  \nIndividual preferences  \nInteractions with others  \nSequential choices  \nRule based scheduling algorithms  \nDeep generative learning  \nSynthesis  \nData anonymisation  \nUp-sampling  \nObject detection  \nTranslation  \nText generation  \nRegular-human performance  \nDecision support tools  \nTransport  \nEnergy  \nEpidemiology  \nBespoke-rules  \nStatistical choice models  \nNested-logit models  \nHazard models  \nMultinomial-logit models  \nMarkov Chain Monte Carlo  \nSimultaneous model  \nSimulated and sampled choice set  \nTour-based approaches  \nDiscriminative machine learning  \nProfile Hidden Markov Models  \nAggregate distributions  \nMarginal and conditioned distributions  \nEarth Movers Distance (EMD)  \nL1 distance  \nSample quality  \nCreativity  \nDiversity  \nHomogeneity  \nNovelty  \nConservatism  \nDiscrete Schedule Encoding  \nContinuous Schedule Encoding  \nConvolutional network  \nRecurrent network  \nAuto-regressive predictions  \nRecurrent Neural Networks (RNNs)  \nVariational Auto Encoders (VAEs)  \nGenerative Adversarial Networks (GANs)  \nNormalising flows  \nProbability density estimation  \nKullback-Leibler Divergence (KLD)  \nCross-entropy loss  \nMean squared error  \nTeacher forcing  \nStochasticity  \nTemporal resolution  \nTemporal consistency  \nAggregate realism  \nDisaggregate realism  \nPopulation synthesis  \nFeasibility-diversity trade-off  \nDifferentiable privacy  \nSmart card data  \nGenerative model  \nLanguage models  \nAttention based architecture  \nLong-range dependencies  \nKL divergence  \nLatent layer  \nReconstruction loss  \nAdam for gradient descent  \nHyper-parameters  \nGrid search  \nTraining datasets  \nDeep machine learning architectures  \nHigh dimensional data  \nImages  \nVideo  \nText  \nSpeech  \nDeep Generative Models (DGMs)  \nDeep generative modelling approach  \nHuman activity scheduling  \nActivity sequences  \nActivity durations  \nActivity start times  \nActivity end times  \nActivity transitions  \nActivity participation rates  \nActivity schedule sequences  \nActivity schedule evaluation  \nActivity schedule encoding  \nActivity schedule generation  \nActivity schedule modelling  \nActivity schedule representation  \nActivity schedule synthesis  \nActivity schedule timing  \nActivity schedule transitions  \nActivity schedule quality  \nActivity schedule creativity  \nActivity schedule density estimation  \nActivity schedule sample quality  \nActivity schedule creativity evaluation  \nActivity schedule encoding  \nActivity schedule framework  \nActivity schedule methodology  \nActivity schedule results  \nActivity schedule discussion  \nActivity schedule conclusions  \nActivity schedule further research  \nActivity schedule acknowledgements  \nActivity schedule references"
  },
  "SEANN A Domain-Informed Neural Network for.pdf": {
    "response": "arXiv:2501.10273v1\nSEANN\nDomain-Informed Neural Network\nEpidemiological Insights\nJean-Baptiste Guimbaud\nMarc Plantevit\nLéa Maître\nRémy Cazabet\nLaboratory of Image Informatics and Information Systems (LIRIS)\nBarcelona Institute for Global Health (ISGlobal)\nMeersens\nEPITA Research Laboratory (LRE)\nlogistic regression\nlinear regression\nparametric models\nnon-parametric machine learning\ndeep neural networks (DNNs)\nexplainable AI (XAI)\nhigh-quality data\nhigh-quantity data\nPooled Effect Sizes (PES)\nMeta-Analysis\nscientific consensus\ncustom loss\ngeneralizability\npredictive performances\nscientific plausibility\none-exposure-one-health-effect\nexposome paradigm\nenvironmental exposures\nurban, chemical, lifestyle, social hazards\nhigh-dimensional data\nobservational studies\nbiostatistical methods\ninterpretability\ndata-driven approaches\nspurious associations\nnatural laws\nbiological pathways\nInformed Machine Learning (IML)\ndomain-specific knowledge\nspatial invariances\nprobabilistic relations\nknowledge graphs\nPhysics-Guided Neural Network (PGNN)\nDomain-Adapted Neural Network (DANN)\nexposome risk scores\nCardiovascular Literature-Based Risk Algorithm (CALIBRA)\nSummary Effects Adapted Neural Network\nOdd Ratios (ORs)\nRelative Risks (RRs)\nStandard Regression Coefficients (SRCs)\ncustom loss function\nperturbation\nShapley values\ncoefficient of determination (R2)\nreceiver operating characteristic curve (ROC)\narea under the curve (AUC)\nGaussian noise\nmean imputation\nearly stopping\nPyTorch\nHELIX project\nHuman Early Life Exposome (HELIX) study"
  },
  "The Distributed Multi-User Point Function.pdf": {
    "response": "Distributed Multi-User Point Function  \nInformation-theoretic distributed multi-user point function  \nTrusted master node  \nServer nodes  \nUsers  \nPoint function  \nFunctional shares  \nCorrectness and information-theoretic privacy constraints  \nCapacity  \nMaximum achievable rate  \nDistributed systems  \nCloud computing  \nBlockchain technology  \nDistributed learning  \nSecret-sharing schemes  \nInformation-theoretic security  \nFunction secret sharing schemes  \nComputational privacy  \nInformation-theoretic privacy  \nPrivate information retrieval (PIR)  \nDistributed key generation (DKG)  \nNon-linear and linear reconstruction methods  \nCommunication and storage complexity  \nVerifiable information-theoretic secure scheme  \nMulti-user or multi-secret sharing  \nDistributed multi-secret or multi-user secret sharing (DMUSS)  \nWeak secrecy conditions  \nPerfect secrecy  \nCapacity region  \nGraph-based secret sharing  \nCorrelated random variables  \nSymmetric and multilevel secret sharing schemes  \nSecure storage systems  \nDistributed data management  \nSecure distributed computation  \nSecret sharing  \nVerifiable information-theoretic point function"
  },
  "The_Evolution_of_Large_Language_Model_Models_Applications_and_Challenges.pdf": {
    "response": "Large Language Models (LLMs)  \nNatural Language Processing (NLP)  \nTransformer architecture  \nBERT  \nGPT  \nText generation  \nTranslation  \nSummarization  \nT5  \nBART  \nBioGPT  \nAccuracy  \nPerplexity  \nBLEU score  \nROUGE score  \nBias  \nOverfitting  \nReal-time processing  \nFederated learning  \nContinuous learning  \nMultimodal LLMs  \nInterpretive AI  \nContext length improvements  \nFine-tuning  \nMulti-modal LLMs  \nRecurrent Neural Networks (RNNs)  \nSelf-attention mechanism  \nPre-trained models  \nDomain-specific applications  \nEthical considerations  \nFuture directions  \nMachine learning  \nArtificial intelligence (AI)  \nNeural networks  \nSelf-supervised learning  \nStatistical approaches  \nN-gram models  \nSequential data modeling  \nVanishing gradients  \nLong-term dependencies  \nParameter initialization  \nOptimization  \nIterative training  \nSpecialized domains  \nPeer-reviewed articles  \nTechnical complexities  \nTaxonomy  \nArchitectures  \nAPI applications  \nSocietal impacts  \nSafety  \nPrivacy  \nEconomics  \nEnvironmental concerns  \nBiomedical  \nHealthcare  \nEducation  \nSocial media  \nBusiness  \nAgriculture  \nArtificial neural networks (ANNs)  \nRule-based systems  \nProbabilistic techniques  \nWord embeddings  \nVector space  \nSemantic relationships  \nGoogle Neural Machine Translation (GNMT)  \nBidirectional Encoder Representations from Transformers  \nGenerative Pre-trained Transformers  \nParameter count  \nText synthesis  \nQuestion-answering  \nSentiment analysis  \nTextual data  \nContextual understanding  \nMachine translation  \nTextual and visual data  \nInnovation  \nExperimentation  \nInterdisciplinary collaboration  \nHypothesis generation  \nProof verification  \nRobo-advising  \nAlgorithmic trading  \nLow-code solutions  \nHuman-robot interaction  \nTask planning  \nMotion planning  \nNavigation  \nObject manipulation  \nModel scaling  \nPretrained language models  \nText-to-text model  \nOpenAI  \nGoogle AI  \nBiomedical text analysis  \nText summarization  \nMultilingual capabilities  \nFactual language understanding  \nMultimodal capabilities  \nReal-world applications  \nTraining data  \nFine-tuning  \nPerformance evaluation  \nEvaluation methods  \nCross-Entropy  \nN-gram precision  \nBrevity penalty  \nSentence order  \nSummarization models  \nN-gram overlap  \nLongest common subsequence (LCS)  \nF1-score  \nBias and fairness  \nMemorization vs. generalization  \nHallucinations  \nPrompt engineering  \nOutdated information  \nHardware acceleration  \nModel optimization  \nDynamic evaluation metrics  \nPersonalization  \nEthical frameworks  \nHuman-AI collaboration  \nResearch and development  \nEthical AI governance  \nRecent breakthroughs  \nTechnical questions"
  }
}