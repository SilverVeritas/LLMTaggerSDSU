{
  "Counterfactual Explanations for k-means and Gaussian Clustering.pdf": "The document presents a novel approach to generating counterfactual explanations for clustering solutions, specifically for k-means and Gaussian clustering. Counterfactual explanations, traditionally used for classifier decisions, are adapted to explain clustering by identifying minimal changes needed to alter cluster assignments. The authors propose a general definition for counterfactuals in model-based clustering, incorporating plausibility and feasibility constraints. For k-means clustering, they provide analytical formulas to compute optimal solutions, while for Gaussian clustering, they require solving a nonlinear equation. The method accounts for actionable and immutable features and allows for specifying a plausibility factor to control the counterfactual's position relative to the cluster boundary. The approach is demonstrated through examples and experimental comparisons, showing its effectiveness in generating counterfactuals with minimal distance from factual instances. The paper also compares the proposed method with existing counterfactual generation techniques for classification, highlighting its advantages in terms of optimality, effectiveness, and computational efficiency. Future work may explore other distance measures and cluster models.",
  "Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based.pdf": "The document is a preprint version of a paper to be published at the ICAACE 2025 conference, focusing on the use of Generative Adversarial Networks (GANs) for credit risk identification in supply chains. The study addresses the critical issue of credit risk management, which is vital for operational stability and financial sustainability in supply chains. It highlights the limitations of traditional risk assessment methods like logistic regression and decision trees, which struggle with dynamic and sequential data. The paper proposes a GAN-based model that generates synthetic credit risk scenarios to enhance predictive accuracy, especially in data-scarce environments. The research evaluates the model across three industries—manufacturing (steel), distribution (pharmaceuticals), and services (e-commerce)—demonstrating that the GAN-based model outperforms traditional methods in accuracy, recall, and F1 scores. The study underscores the potential of GANs in proactive risk management and suggests future research could incorporate external market factors to further improve predictive capabilities. The document is relevant for categorizing research in machine learning applications in supply chain finance, specifically focusing on credit risk management using GANs.",
  "Grey-Box Fuzzing in Constrained Ultra-Large Systems Lessons.pdf": "The document discusses \"SandBoxFuzz,\" a scalable grey-box fuzzing technique designed for testing ultra-large microservices-based FinTech systems, particularly in constrained industrial environments like Ant Group. Traditional fuzzing methods face challenges in such settings due to restricted access to production environments, complex dependencies, and stringent security constraints. SandBoxFuzz addresses these issues by using aspect-oriented programming and runtime reflection for dynamic specification mining, which helps generate targeted inputs for constrained environments. It also introduces a log-based coverage mechanism integrated into the build pipeline, eliminating the need for runtime coverage agents. Compared to an initial solution, FatFuzz, SandBoxFuzz shows superior performance, achieving a 7.5% increase in branch coverage, identifying 1,850 additional exceptions, and reducing setup time from hours to minutes. The document highlights the challenges of applying fuzz testing in industrial settings, such as limited permissions, restricted test environments, and input construction constraints. SandBoxFuzz addresses these by using a sandbox approach to intercept inputs at specific pointcuts during the testing process, allowing for more efficient and scalable fuzzing in real-world industrial contexts. The tool has been open-sourced to aid researchers and practitioners in testing large-scale microservices systems.",
  "Hybrid Deep Learning Model for epileptic seizure.pdf": "The document presents a study on a novel hybrid deep learning model for epileptic seizure detection using EEG signals. The model integrates wavelet transform, 1D convolutional layers, and a multi-head attention mechanism to enhance the accuracy of seizure prediction. The model achieved a classification accuracy of 99.83% on benchmark datasets, outperforming existing models. The study emphasizes the importance of the attention mechanism in capturing temporal features in EEG data. The research utilizes the UCI Epileptic Seizure Dataset, restructured for binary classification, and applies various regularization techniques to prevent overfitting. The model's performance is evaluated using metrics like accuracy, precision, recall, F1 score, CSI, and MCC. The study concludes that the integration of CNNs and attention mechanisms significantly improves the detection of epileptic seizures, with potential applications in clinical settings.",
  "Mapping the Increasing Use of LLMs in Scientific Papers.pdf": "The document is a preprint study titled \"Mapping the Increasing Use of LLMs in Scientific Papers,\" conducted by researchers from Stanford University and UC Santa Barbara. It investigates the growing use of large language models (LLMs), such as ChatGPT, in academic writing. The study analyzes 950,965 papers from arXiv, bioRxiv, and Nature portfolio journals, published between January 2020 and February 2024, using a statistical framework to estimate the prevalence of LLM-modified content. The findings indicate a steady increase in LLM usage, particularly in Computer Science papers, where up to 17.5% of content is LLM-modified. In contrast, Mathematics and Nature portfolio papers show the least modification. The study also finds that higher LLM usage is associated with papers from authors who frequently post preprints, papers in crowded research areas, and shorter papers. The research highlights the implications of LLM use on scientific publishing, including concerns about accuracy, plagiarism, and linguistic discrimination. The study employs a distributional GPT quantification framework to estimate LLM-modified content at a population level, offering insights into structural motivations for LLM use and potential risks to the scientific publishing ecosystem.",
  "MechIR A Mechanistic Interpretability Framework.pdf": "The document introduces MechIR, a mechanistic interpretability framework specifically designed for information retrieval (IR) tasks. Mechanistic interpretability is a diagnostic approach that aims to make the internal workings of neural models more transparent by identifying causal relationships between model components and outputs. This is particularly important as neural models become more prevalent in IR, where understanding model behavior is crucial for transparency and system improvement.\n\nMechIR provides a flexible framework for analyzing and intervening in neural IR models, allowing researchers to reverse-engineer these models and investigate their internal components causally. The framework includes tools for activation patching, a method that identifies specific model components responsible for certain behaviors by comparing model performance on perturbed and baseline inputs. MechIR extends the functionality of existing tools like TransformerLens to support common IR architectures, such as bi-encoders and cross-encoders.\n\nThe document also highlights the potential applications of MechIR, including enhancing model performance, mitigating bias, and preventing adversarial attacks. It targets PhD students and researchers in explainable information retrieval (XIR) and aims to encourage further research and collaboration in mechanistic interpretability within the IR community. The framework is available as an open-source Python package, complete with documentation and tutorials to assist both novice and experienced researchers.",
  "Modelling Activity Scheduling Behaviour with Deep.pdf": "The document presents a novel approach to modeling human activity scheduling behavior using deep generative machine learning, specifically through Variational Auto-Encoders (VAEs). The authors, Fred Shone and Dr. Tim Hillel, propose a new method for encoding activity schedules with continuous activity durations and introduce a comprehensive framework for evaluating the quality of generated schedules. The study highlights the importance of activity scheduling in domains like transport, energy, and epidemiology, where realistic human behavior modeling is crucial.\n\nThe document critiques existing methods that decompose scheduling into sequential choices, which may not capture the true diversity and complexity of human behavior. The authors argue that their approach, which synthesizes large, diverse, and realistic samples of activity schedules, offers simplicity, speed, and improved interaction of choice components.\n\nTwo VAE models are evaluated: a Discrete CNN and a Continuous RNN, each with different encoding strategies. The Discrete CNN treats schedules like images, while the Continuous RNN treats them like text sequences. Both models are assessed on their ability to replicate real-world activity schedules, with the Continuous RNN showing better performance in activity participation and transitions, while the Discrete CNN excels in timing accuracy.\n\nThe study concludes that deep generative models can effectively approximate complex human behavioral systems, offering a practical and scalable solution for generating realistic activity schedules. The authors suggest further research to enhance model capabilities, such as incorporating socio-economic attributes and extending the complexity of generated schedules.",
  "SEANN A Domain-Informed Neural Network for.pdf": "The document introduces SEANN, a novel domain-informed neural network designed to enhance epidemiological insights by integrating Pooled Effect Sizes (PES) into deep neural networks (DNNs). Traditional statistical methods in epidemiology often struggle with high-dimensional data and limited datasets, while DNNs, despite their potential, face challenges due to data scarcity and lack of interpretability. SEANN addresses these issues by incorporating PESs, which are quantitative forms of scientific consensus derived from meta-analyses, directly into the learning process through a custom loss function. This integration aims to improve the generalizability and scientific plausibility of the model's predictions, especially in noisy and scarce data settings.\n\nThe paper details the methodology for integrating different types of PESs, such as standardized regression coefficients, odd ratios, and risk ratios, into the neural network's training process. It also presents experimental validations using synthetic data to demonstrate SEANN's ability to maintain predictive performance and capture scientifically plausible relationships, even with imperfect data. The experiments show that SEANN can better disentangle individual effects and estimate the impact of unknown confounding factors compared to agnostic DNNs.\n\nOverall, SEANN offers a promising approach to leverage existing scientific knowledge in the form of PESs to enhance the reliability and interpretability of machine learning models in epidemiology, potentially improving the predictive power of environmental clinical risk scores.",
  "The Distributed Multi-User Point Function.pdf": "The document introduces the concept of a Distributed Multi-User Point Function (DMUPF), which integrates multi-user secret sharing and information-theoretic distributed point functions into a single protocol. The system involves a master node, multiple server nodes, and users, where each user can access a subset of server nodes to retrieve their desired point function result while maintaining privacy. The scheme is designed to ensure that users can recover their point function results without gaining information about other users' functions, even with access to shared server responses. The paper presents a novel multi-user scheme that satisfies correctness and information-theoretic privacy constraints, characterizing the inner and outer bounds on the capacity of the DMUPF scheme. The capacity is defined as the maximum achievable rate relative to the storage size of the servers. The authors propose a four-phase scheme (placement, demand, evaluation, and retrieval) and demonstrate its correctness and privacy through novel mapping functions and the use of DMUSS (Distributed Multi-User Secret Sharing) schemes. The paper concludes by discussing potential future research directions, including exploring perfect privacy conditions and extending the scheme to multi-point functions.",
  "The_Evolution_of_Large_Language_Model_Models_Applications_and_Challenges.pdf": "The document provides a comprehensive overview of Large Language Models (LLMs), focusing on their architecture, applications, challenges, and future directions. It traces the evolution of LLMs from early rule-based systems to advanced transformer architectures like BERT and GPT, highlighting their success in natural language processing tasks such as text generation, translation, and summarization. Key LLMs like T5, BART, and BioGPT are discussed, along with performance evaluation metrics like accuracy, perplexity, BLEU, and ROUGE scores. The paper addresses challenges such as bias, overfitting, and real-time processing, and suggests future directions including managing longer contexts, reducing bias, and improving efficiency through methods like federated learning. The document also explores the applications of LLMs across various domains, including medicine, education, and finance, and emphasizes the importance of ethical considerations in their development and deployment."
}